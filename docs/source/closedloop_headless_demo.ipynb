{
 "cells": [
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Closed-loop acquisition and perturbation with pycro-manager\n",
    "\n",
    "### an example of closed-loop experimentation enabled by pycro-manager\n",
    "\n",
    "When imaging live biological samples, we often have specific features of cellular activity we are interested in, such as a pattern of neural activity or stage in the cell cycle. We can interrogate these dynamics with closed-loop (CL) experimental design. CL perturbations are triggered by signals derived from data acquired from the sample itself during a live recording session. Recent advancements in computing allow experimenters to coduct closed-loop experiments, which will deeply influence optical physiology, allowing realtime adaptation to animal state, enforcement of physiological constraints on evoked patterns, calibrated control with cellular resolution, and a variety of important experimental controls that were previously inaccessible (Grosenick, Marshel, and Deisseroth 2016 Neuron). Specifically, CL experiments:\n",
    "\n",
    "* ensure perturbation occurs during statistically rare conditions\n",
    "* allow online tuning of optogenetic inputs in vivo (to achieve specific output parameters)\n",
    "* allow online system identification / modeling of neural circuits (i.e. causally establish functional circuit architecture)\n",
    "* steer the system into desired or otherwise non-observable states\n",
    "* eliminate off-target effects of non-closed-loop perturbations\n",
    "* reduce variability of system state at time of stimulus onset\n",
    "\n",
    "In this example we use features of pycro-manager which enable closed-loop experimentation. Specifically we perform some canonical image processing (template filtering with 2d gaussian kernel, thresholding, median filtering), then find local peaks, then take a window of pixel values around each peak. We use these pixel values to trigger our arbitrary \"stimulus\" function which can e.g. change optical settings on the microscope, call a separate program, etc.\n",
    "\n",
    "  \n",
    "\n",
    "Here we use snap_image() to acquire our images for readability and to show an example of headless pycromanager acquisition. Alternatively one could use pycro-manager Acquisitions to run our closed-loop experiment. We also leverage a few neat tricks:\n",
    "\n",
    "* we strobe our imaging acquisition by introducing a small delay between images. This makes snap_image() timing an order of magnitude more accurate, and reflects a common imaging condition for perturbative experiments, and gives our closed-loop processing algorithm time to perform computation.\n",
    "* we use the python package numba to just-in-time compile our closed-loop computation into LLVM intermediate representation. This affords an order-of-magnitude speedup, as numba-compiled numerical algorithms can allow Python code to approach the speeds of C.\n",
    "\n",
    "By Raymond L. Dunn, the FOCO Lab, UC San Francisco\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### code\n",
    "load pycro-manager objects and parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple single image acquisition example with snap\n",
    "from pycromanager import Bridge\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#### Setup ####\n",
    "bridge = Bridge()\n",
    "core = bridge.get_core()\n",
    "\n",
    "#### imaging settings\n",
    "exposure = 20\n",
    "num_frames_to_capture = 100\n",
    "core.set_exposure(exposure)\n",
    "\n",
    "#### strobe settings\n",
    "# by enforcing a strobe (a small delay between acquisitions), our snap_image acquisition framerate becomes an order of magnitude more accurate (as of 20201006)\n",
    "interframe_interval = 50\n",
    "assert interframe_interval > exposure\n",
    "\n",
    "#### holder variables for images, model values, and processing timestamps\n",
    "frames = []\n",
    "model = []\n",
    "acq_timestamps = []\n",
    "process_timestamps = []\n"
   ]
  },
  {
   "source": [
    "define an image quantification function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have this module downloaded and in the appropriate directory so you can import it\n",
    "# you might have to install some other python dependencies\n",
    "import ImageProcessorFOCO as ImageProcessor\n",
    "\n",
    "# define your image processing function\n",
    "# in this case we're doing some image processing, finding local peaks, and taking a 3x3 grid of pixel values from each peak\n",
    "# this function returns whether or not to trigger stimulation\n",
    "def process_frame(frame, ip, is_demo=False):\n",
    "\n",
    "    # if we're running this example with the micromanager demo config, peakfinding doesn't really make sense on gratings\n",
    "    if is_demo:\n",
    "        return 0\n",
    "\n",
    "    # simple peakfinding algorithm from accompanying module\n",
    "    xys_list = ip.segmentchunk(frame.astype(np.float32))\n",
    "\n",
    "    # if no peaks, return placeholder value\n",
    "    if len(xys_list) == 0:\n",
    "        return 0\n",
    "\n",
    "    # grab 3x3 pixels around each peak\n",
    "    pix = []\n",
    "    xys = np.array(xys_list) - 1  # -1 because of single pixel offset bug...\n",
    "    pix.append(frame[xys[:, 0], xys[:, 1]])\n",
    "    pix.append(frame[xys[:, 0], xys[:, 1] - 1])\n",
    "    pix.append(frame[xys[:, 0], xys[:, 1] + 1])\n",
    "    pix.append(frame[xys[:, 0] - 1, xys[:, 1]])\n",
    "    pix.append(frame[xys[:, 0] - 1, xys[:, 1] - 1])\n",
    "    pix.append(frame[xys[:, 0] - 1, xys[:, 1] + 1])\n",
    "    pix.append(frame[xys[:, 0] + 1, xys[:, 1]])\n",
    "    pix.append(frame[xys[:, 0] + 1, xys[:, 1] - 1])\n",
    "    pix.append(frame[xys[:, 0] + 1, xys[:, 1] + 1])\n",
    "\n",
    "    # flatten and sort peak-averages\n",
    "    peak_averages = np.sort(np.array(pix).mean(axis=0).flatten())\n",
    "\n",
    "    # in this example let's just average across peaks\n",
    "    avg = peak_averages.mean()\n",
    "\n",
    "    return avg\n",
    "\n",
    "#### quantification settings\n",
    "# initialize jit precompilation with an intial image from the microscope\n",
    "ip = ImageProcessor.ImageProcessor()\n",
    "core.snap_image()\n",
    "tagged_image = core.get_tagged_image()\n",
    "frame = np.reshape(tagged_image.pix, newshape=[tagged_image.tags['Height'], tagged_image.tags['Width']])\n",
    "garbage = process_frame(frame, ip)\n"
   ]
  },
  {
   "source": [
    "define a function for how your real-time quantified data triggers e.g. a microfluidic solenoid or a laser\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this demo we have a dummy function (it's over 9000 lol)\n",
    "def process_model(model):\n",
    "\n",
    "    threshold = 9000\n",
    "    if model[-1] > threshold:\n",
    "        \n",
    "        # code here to do whatever perturbation you want\n",
    "        pass\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "source": [
    "run acquisition. iteratively take frames, quantify, and check for stimulation trigger "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### do acquisition\n",
    "print('beginning acquisition...')\n",
    "t0 = time.time()\n",
    "next_call = time.time()  # updated periodically for when to take next image\n",
    "for f in range(num_frames_to_capture):\n",
    "\n",
    "    # snap image\n",
    "    core.snap_image()\n",
    "    tagged_image = core.get_tagged_image()\n",
    "    \n",
    "    # save acquisition time timestamp\n",
    "    t1 = time.time()\n",
    "    acq_timestamps.append(time.time() - t0)\n",
    "\n",
    "    # pixels by default come out as a 1D array. We can reshape them into an image\n",
    "    frame = np.reshape(tagged_image.pix, newshape=[tagged_image.tags['Height'], tagged_image.tags['Width']])\n",
    "\n",
    "    # quantify image and save processing time timestamp\n",
    "    val = process_frame(frame, ip, is_demo=True) \n",
    "    process_timestamps.append(time.time() - t1)\n",
    "\n",
    "    # store latest value in model and conditionally trigger perturbation\n",
    "    model.append(val)\n",
    "    process_model(model)\n",
    "\n",
    "    # helpful printout to monitor progress\n",
    "    if f%50 == 0:\n",
    "        print('current frame: {}'.format(f))\n",
    "\n",
    "    # wait until we're ready to snap next image. note that in this example, the first few images may exceed the strobe delay as numba jit compiles the relevant python functions\n",
    "    nowtime = time.time()\n",
    "    next_call = next_call + interframe_interval / 1000\n",
    "    if next_call - nowtime < 0:\n",
    "        print(\"warning: strobe delay exceeded inter-frame-interval on frame {}.\".format(f))\n",
    "    else:\n",
    "        time.sleep(next_call - nowtime)\n",
    "\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('thanks for reading!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}